mode: rescore
diffusion: absorbing_state
backbone: dit
parameterization: subs
time_conditioning: false
T: 0
subs_masking: false
seed: 1
loader:
  global_batch_size: 512
  eval_global_batch_size: ${.global_batch_size}
  batch_size: ${div_up:${.global_batch_size}, ${eval:${trainer.devices} * ${trainer.num_nodes}}}
  eval_batch_size: ${div_up:${.eval_global_batch_size}, ${eval:${trainer.devices}
    * ${trainer.num_nodes}}}
  num_workers: ${eval:"len(__import__('os').sched_getaffinity(0))"}
  pin_memory: true
sampling:
  predictor: ddpm_cache
  steps: 128
  noise_removal: true
  num_sample_batches: 2
  num_sample_log: 2
  semi_ar: false
  stride_length: 1
  num_strides: 1
training:
  ema: 0.9999
  antithetic_sampling: true
  importance_sampling: false
  sampling_eps: 0.001
  change_of_variables: false
eval:
  checkpoint_path: /rwthfs/rz/cluster/home/ra717140/setups/exp2025-08-22/work/diffusion_lm_2025/jobs/train_diffusion_lm_mdlm2024/TrainDiffusionLMJob.D7pTK4qTuxa5/output/trained_model_mdlm_librispeech_small/checkpoints/best.ckpt
  disable_ema: false
  compute_generative_perplexity: false
  perplexity_batch_size: 8
  compute_perplexity_on_sanity: false
  gen_ppl_eval_model_name_or_path: gpt2-large
  generate_samples: true
optim:
  weight_decay: 0
  lr: 0.0003
  beta1: 0.9
  beta2: 0.999
  eps: 1.0e-08
trainer:
  _target_: lightning.Trainer
  accelerator: cuda
  num_nodes: 1
  devices: ${device_count:}
  accumulate_grad_batches: ${div_up:${loader.global_batch_size}, ${eval:${trainer.devices}
    * ${loader.batch_size} * ${trainer.num_nodes}}}
  gradient_clip_val: 1.0
  precision: bf16
  num_sanity_val_steps: 2
  max_steps: 1000000
  log_every_n_steps: 10
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  val_check_interval: 10000
wandb:
  project: text-diffusion
  notes: Mulan for text
  group: null
  job_type: null
  name: null
  id: ${.name}_${seed}
  tags:
  - ${noise.type}
  - ${data.train}
  - ${data.valid}
checkpointing:
  save_dir: ${cwd:}
  resume_from_ckpt: true
  resume_ckpt_path: ${.save_dir}/checkpoints/last.ckpt
rescore:
  hypothesis_file: /rwthfs/rz/cluster/home/ra717140/setups/exp2025-08-22/work/i6_core/returnn/search/SearchRemoveLabelJob.EBQUb3reIa71/output/search_results.py.gz
  output_dir: /rwthfs/rz/cluster/home/ra717140/setups/exp2025-08-22/work/diffusion_lm_2025/jobs/rescore_diffusion_lm_mdlm2024/RescoreDiffusionLMJob.JZDbRRw3pS5C/output/rescore_mdlm-librispeech-small-05m-256-time-conditioning-v2
  batch_size: 16
callbacks:
  checkpoint_every_n_steps:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    save_top_k: 1
    save_last: true
    dirpath: ${checkpointing.save_dir}/checkpoints
    verbose: true
    auto_insert_metric_name: false
    every_n_train_steps: 500
  checkpoint_monitor:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: val/nll
    mode: min
    save_top_k: 1
    save_last: false
    dirpath: ${checkpointing.save_dir}/checkpoints
    filename: best
    auto_insert_metric_name: false
    verbose: true
  learning_rate_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step
data:
  train: openwebtext
  valid: wikitext103
  tokenizer_name_or_path: gpt2
  cache_dir: /share/kuleshov/ssahoo/textdiffusion/data
  wrap: true
  streaming: false
model:
  name: small
  type: ddit
  hidden_size: 768
  cond_dim: 128
  length: 1024
  n_blocks: 12
  n_heads: 12
  scale_by_sigma: true
  dropout: 0.1
  tie_word_embeddings: false
strategy:
  _target_: lightning.pytorch.strategies.DDPStrategy
  find_unused_parameters: false
noise:
  type: loglinear
  sigma_min: 0.0001
  sigma_max: 20
lr_scheduler:
  _target_: transformers.get_constant_schedule_with_warmup
  num_warmup_steps: 2500
